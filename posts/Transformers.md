#ml 

Multi Head self attention Mechanism
In this there are three vector spaces Query, Key and Value and we have three matrices W<sub>q</sub> W<sub>k</sub> W<sub>v</sub> matrices which are basically representing the linear transformation into that space so for each sequence of token we compute attention scores and for a token attention score is the product of its query vector with the key vector of all the other tokens   
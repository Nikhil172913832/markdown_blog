#ml 

This is a metric to compare how different are two distributions it is mainly used when you want to perform an approximation for a given data distribution to a known data distribution so that analysis can be made simpler. the formula is submission of the product of the probability of the candidate distribution with the log of the ratio of candidate to the reference distribution. To get the gist of it think it like you asked your friends what fruits they like and you gave them three options lets say apples, oranges and bananas now lets say 50 percent said they liked apples, 40 percent said they like oranges and 10 said banana now you asked them the same question to other set of friends and now lets say you want to compare how different is the distribution of the one set of friends to other you might think of taking the average ratio for each entity but that might not be representative of the shift as mean of a set of numbers is skewed as a large value (an outlier) will disturb the whole parameter so we need something that can help scale these ratios and then we can take the mean to have a robust parameter and for that we might wanna use logarithmic function and now just taking the mean might not be ideal as you are taking equal contribution from change of each entity let say the other friend group the candidate distribution has certain points which have significantly more importance or for the example say they like a certain fruit more so you would naturally want to give those features more so the best way to do that is you multiply the log term with the probability of each log term. 
the mathematical formula is -
is submission over all the unique values of product of probability in the candidate distribution to the log of the ratio of candidate by reference distribution.
Just clarify that KL measures the **relative entropy** and isn't a true "distance" metric due to asymmetry.
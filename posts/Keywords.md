#ml 

1. Out-of-core algorithm - Algorithm where the whole dataset is not loaded into memory but mini-batches are loaded first and the model incrementally learns on each batch, helpful when the dataset is large especially when doing offline learning(where the model has to be retrained from scratch with the new data every time it needs to be refreshed).
2. Data Pipelines - A sequence of data processing components is called a data pipeline.
3. l<sub>k</sub> norm - ||v||<sub>k</sub> = (|v<sub>0</sub>|<sup>k</sup> +|v<sub>1</sub>|<sup>k</sup> + ... + |v<sub>n</sub>|<sup>k</sup>)<sup>1/k</sup> 
4. F1 score: 2TP/(2TP + FP + FN)
5. IoU(Intersection over Union): TP/(TP + FP + FN)
6. - **Intersection over Union (IoU):** IoU is a measure that quantifies the overlap between a predicted bounding box and a ground truth bounding box. It plays a fundamental role in evaluating the accuracy of object localization. 
7. **Average Precision (AP):** AP computes the area under the precision-recall curve, providing a single value that encapsulates the model's precision and recall performance.
8. **Mean Average Precision (mAP):** mAP extends the concept of AP by calculating the average AP values across multiple object classes. This is useful in multi-class object detection scenarios to provide a comprehensive evaluation of the model's performance.
9. **Precision and Recall:** Precision quantifies the proportion of true positives among all positive predictions, assessing the model's capability to avoid false positives. On the other hand, Recall calculates the proportion of true positives among all actual positives, measuring the model's ability to detect all instances of a class.
10.  **F1 Score:** The F1 Score is the harmonic mean of precision and recall, providing a balanced assessment of a model's performance while considering both false positives and false negatives.
11. Neural Turing Machine: 
12. n-gram: An n-gram is a sequence of n adjacent items in a text document. The items can be words, numbers, symbols, or punctuation.
13. 
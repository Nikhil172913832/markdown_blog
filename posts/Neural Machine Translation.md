#ml 

1. The previous approach to machine translation includes a basic encoder-decoder architecture where the encoder converts the sentence from the source language to a fixed length vector and then the decoder converts the fixed length vector to the target language. The system of encoder-decoder is jointly trained to maximise the probability of a correct translation but the assumption we are making here is that the encoder will be able to squeeze in all the necessary details into the fixed length vector, which is not possible for longer sentences, especially the one that are longer than the sentences in the training corpus.
2. So to deal with the problem we create an extension to this encoder-decoder architecture where the proposed model generates a word and then finds positions which are most relevant to the word and calculate the attention weights for these which is done by feeding the current decoder's state and the encoded inputs to a feed forward network which generates weights which tell us how relevant is a word for generating the next word and then we take the weighted sum of encoded vectors with the attention weights which gives us the context vector.So as these attention weights are calculated for each output word the model adaptively focuses on different parts of the input sentence as it generates the translation.
3. So, probabilistic-ally speaking neural machine translation task is similar to maximising the conditional probability of the resultant given the source sentence.
4. The most common approach is to use an RNN for encoder-decoder architecture such that h<sub>t</sub> = f(x<sub>t</sub>, h<sub>t-1</sub>) where h is the hidden state and x is input and the subscript signifies the time stamp.
6. p(y<sub>i</sub>|y<sub>1</sub>, . . . , y<sub>i−1</sub>, x) = g(y<sub>i−1</sub>, s<sub>i</sub>, c<sub>i</sub>)
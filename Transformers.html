<!DOCTYPE html>
<html>
<head><meta charset="UTF-8"><title>Transformers</title></head>
<body><h1>ml</h1>
<p>Multi Head self attention Mechanism
In this there are three vector spaces Query, Key and Value and we have three matrices W<sub>q</sub> W<sub>k</sub> W<sub>v</sub> matrices which are basically representing the linear transformation into that space so for each sequence of token we compute attention scores and for a token attention score is the product of its query vector with the key vector of all the other tokens   </p></body>
</html>